{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_file=\"embeddings_full100_100_final.txt\"\n",
    "#embeddings_file=\"../../ull_ed/practical-2/embeddings_final_50_100_hansards_bsg.txt\"\n",
    "#embeddings_file=\"../../ull_ed/practical-2/embeddings_vocab_1613_ep_50_100_hid_100_test_bsg.txt\"\n",
    "pretrained = True\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    embeddings_file=\"/Users/efiathieniti/Downloads/bow2.words\"\n",
    "    null_word = 'null'\n",
    "else:\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1613_ep_20_100_hid_100_test_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1501_ep_40_emb_50_hid_100_True_test_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1601_ep_40_emb_100_hid_100_1600_test_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1551_ep_20_emb_100_hid_100_8_test_test_1550_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1401_ep_2_emb_100_hid_100_1400_test_bsg.txt\"\n",
    "\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1601_ep_8_emb_100_hid_100_4_test_test_1600_cat_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1601_ep_40_emb_100_hid_100_1600_test_bsg_good_with_common.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_1601_ep_8_emb_100_hid_100_128_test_test_1600_cat_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/embeddings_final_100_100_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_5533_ep_2_emb_100_hid_100_128_hansards_test_7000_cat_bsg.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_skipgram.txt\"\n",
    "    embeddings_file=\"../ull_ed/practical-2/output/embeddings_vocab_5001_5000_epochs_10_skipgram.txt\"\n",
    "    \n",
    "    null_word = '<null>'\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "#word_vectors = KeyedVectors.load_word2vec_format('../../../ULL/data/bow2.words.bz2', binary=False)  # C text format\n",
    "word_vectors = KeyedVectors.load_word2vec_format(embeddings_file, binary=False, encoding='utf-8')  # C binary format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation against Simlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors.evaluate_word_pairs(\"../gensim/gensim/test/test_data/wordsim353.tsv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "path = '../../../Downloads/lst/'\n",
    "lst_test_f = 'lst_test.preprocessed'\n",
    "ls_candidates = 'lst.gold.candidates'\n",
    "with open(path+lst_test_f,) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    sentences = []\n",
    "    for row in reader:\n",
    "        sentences.append(row)\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# lstf file\n",
    "# Columns: target word, id_sentence,    position of target word in sentence,    sentence\n",
    "\n",
    "# Create the sentences to replace\n",
    "# with open(path + lstf) as csvfile:\n",
    "#     reader = csv.reader(csvfile)\n",
    "#     sentences = []\n",
    "#     for row in reader:\n",
    "#         #sentences.append(row.split(\"\\t\"))\n",
    "#         print(row)\n",
    "\n",
    "sentences_df = pd.read_csv(path + lstf, sep=\"\\t\",  header=None, quoting=csv.QUOTE_NONE,\n",
    "                       names = [\"target\",\"id_sentence\",\"position\",\"sentence\"])\n",
    "\n",
    "\n",
    "# Create the candidates dictionary\n",
    "word_to_candidates = {}\n",
    "with open(path+ls_candidates, 'r') as f:\n",
    "    candidates = f.read().strip().split('\\n')\n",
    "    candidates = [tuple(line.split('::')) for line in candidates]\n",
    "    for word, candids in candidates:\n",
    "        cands = candids.split(';')\n",
    "        w_cands = []\n",
    "        for w in cands:\n",
    "            if not ' ' in w:\n",
    "                w_cands.append(w)\n",
    "        word_to_candidates[word] = w_cands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenized = []\n",
    "contexts = []\n",
    "for sent in sentences_df.sentence:\n",
    "    tokenized.append(tokenizer.tokenize(sent))\n",
    "    \n",
    "sentences_df[\"tokenized\"] = tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the contexts based on window size\n",
    "window_size = 5\n",
    "contexts=[]\n",
    "counter=0\n",
    "\n",
    "for index, row in sentences_df.iterrows():\n",
    "    pos = int(row[\"position\"])\n",
    "    target = row[\"target\"]\n",
    "    sent = row[\"tokenized\"]\n",
    "    words = [w if (w in word_vectors and not w.isdigit()) else null_word for w in sent]\n",
    "    context = words[max(0, pos - window_size):min(len(words), pos + window_size)]\n",
    "    contexts.append(context)\n",
    "sentences_df['contexts'] = contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def write_rankings( rankings):\n",
    "        out = ''\n",
    "        for index, target, ranking in rankings:\n",
    "            out += '#RANKED\\t {} {}\\t'.format(target, index)\n",
    "            for candidate, score in ranking:\n",
    "                out += '{} {}\\t'.format(candidate, score)\n",
    "            out = out[:-1] + '\\n'\n",
    "        with open('data/lst.out', 'w+') as f:\n",
    "            f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [\"target\",\"id_sentence\",\"position\",\"sentence\", \"contexts\"]\n",
    "from operator import itemgetter\n",
    "\n",
    "rankings = []\n",
    "for index, row in sentences_df.iterrows():\n",
    "    target = row[\"target\"].split(\".\")[0]\n",
    "    scores = []\n",
    "    if target in word_vectors:\n",
    "        for candidate in word_to_candidates[row[\"target\"]]:\n",
    "            if candidate not in word_vectors: cand_vec\n",
    "            cand_vec = word_vectors[candidate] if candidate in word_vectors else word_vectors[null_word]\n",
    "            word_vec = word_vectors[target] # obtain without .n\n",
    "            # score against central word\n",
    "            #np.random.shuffle(cand_vec)\n",
    "            #np.random.shuffle(word_vec)\n",
    "            score = analyze.cosine_similarity(cand_vec, word_vec)\n",
    "            for context in row.contexts:\n",
    "                context_vec= word_vectors[context]\n",
    "                #np.random.shuffle(context_vec)\n",
    "\n",
    "                score+=analyze.cosine_similarity(cand_vec, context_vec)\n",
    "                score /= (len(row.contexts) + 1)\n",
    "\n",
    "            # score for each candidate\n",
    "            scores.append((candidate,score))\n",
    "        scores.sort(key=itemgetter(1), reverse=True)\n",
    "        rankings.append((row[\"id_sentence\"],row[\"target\"], scores))\n",
    "    else:\n",
    "        rankings.append((row[\"id_sentence\"],row[\"target\"],[]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rankings(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
